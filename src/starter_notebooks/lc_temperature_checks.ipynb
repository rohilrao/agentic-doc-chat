{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970b01b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "957b2ab3",
   "metadata": {},
   "source": [
    "## LC Prompt Self consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f473b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sequential Requests:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sequential Requests: 100%|██████████| 100/100 [00:10<00:00,  9.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 11.00 seconds\n",
      "Extracted Numbers: [53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "from tqdm import tqdm\n",
    "import time, re\n",
    "\n",
    "llm_main = ChatOllama(\n",
    "    model=\"llama3.1:latest\",\n",
    "    temperature=0.0,\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    api_key=\"ollama\"\n",
    ")\n",
    "\n",
    "random_numbers = []\n",
    "start_time = time.time()\n",
    "\n",
    "for _ in tqdm(range(100), desc=\"Sequential Requests\"):\n",
    "    prompt = HumanMessage(content=\"Give me a random number between 1 and 100.\")\n",
    "    response = llm_main.invoke([prompt])\n",
    "    random_numbers.append(response.content)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Extract numbers\n",
    "numbers = [int(m.group()) for r in random_numbers if (m := re.search(r'\\d+', r))]\n",
    "print(\"Extracted Numbers:\", numbers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a378db6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parallel Requests:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parallel Requests: 100%|██████████| 100/100 [00:06<00:00, 14.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 6.94 seconds\n",
      "Extracted Numbers: [53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import time, re\n",
    "\n",
    "llm_main = ChatOllama(\n",
    "    model=\"llama3.1:latest\",\n",
    "    temperature=0.0,\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    api_key=\"ollama\"\n",
    ")\n",
    "\n",
    "def ask_random_number():\n",
    "    prompt = HumanMessage(content=\"Give me a random number between 1 and 100.\")\n",
    "    try:\n",
    "        response = llm_main.invoke([prompt])\n",
    "        return response.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "NUM_REQUESTS = 100\n",
    "responses = []\n",
    "\n",
    "start_time = time.time()\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = [executor.submit(ask_random_number) for _ in range(NUM_REQUESTS)]\n",
    "    for f in tqdm(as_completed(futures), total=NUM_REQUESTS, desc=\"Parallel Requests\"):\n",
    "        responses.append(f.result())\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Extract numbers\n",
    "numbers = [int(m.group()) for r in responses if (m := re.search(r'\\d+', r))]\n",
    "print(\"Extracted Numbers:\", numbers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f90e38a",
   "metadata": {},
   "source": [
    "## temperature and randomness checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "175d3c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Dog Names:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Dog Names: 100%|██████████| 100/100 [00:16<00:00,  6.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time taken: 16.41 seconds\n",
      "\n",
      "Total names collected: 100\n",
      "Unique names: 1\n",
      "\n",
      "Sample names: [\"The random name I've generated is... **Bramble**!\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# 1. Initialize model\n",
    "llm_main = ChatOllama(\n",
    "    model=\"llama3.1:latest\",\n",
    "    temperature=0.0,  # use 0.0 for deterministic outputs\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    api_key=\"ollama\"\n",
    ")\n",
    "\n",
    "# 2. Collect dog names\n",
    "dog_names = []\n",
    "start_time = time.time()\n",
    "\n",
    "for _ in tqdm(range(100), desc=\"Getting Dog Names\"):\n",
    "    prompt = HumanMessage(content=\"Give me a random name of a dog:\")\n",
    "    response = llm_main.invoke([prompt])\n",
    "    dog_names.append(response.content.strip())\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nTime taken: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# 3. Print summary\n",
    "unique_names = set(dog_names)\n",
    "print(f\"\\nTotal names collected: {len(dog_names)}\")\n",
    "print(f\"Unique names: {len(unique_names)}\")\n",
    "print(\"\\nSample names:\", list(unique_names)[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991d74b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976f3041",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agenticrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

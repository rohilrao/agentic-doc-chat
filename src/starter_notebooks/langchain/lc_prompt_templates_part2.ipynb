{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff3fa1f6",
   "metadata": {},
   "source": [
    "### Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f8903de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_ollama.chat_models import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9bd0ce42",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    model=\"llama3.1:latest\",\n",
    "    temperature=0.0,  # use 0.0 for deterministic outputs\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    api_key=\"ollama\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc81924d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='You might enjoy \"Gone Girl\" by Gillian Flynn, a twisty and suspenseful thriller about a marriage that takes a dark and unexpected turn.' additional_kwargs={} response_metadata={'model': 'llama3.1:latest', 'created_at': '2025-07-25T11:25:30.807778298Z', 'done': True, 'done_reason': 'stop', 'total_duration': 395294223, 'load_duration': 22384847, 'prompt_eval_count': 46, 'prompt_eval_duration': 3591563, 'eval_count': 32, 'eval_duration': 368650813, 'model_name': 'llama3.1:latest'} id='run--f9cb64dd-d80a-448c-9182-4c4f6b549fe3-0' usage_metadata={'input_tokens': 46, 'output_tokens': 32, 'total_tokens': 78}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Consider reading \"Dune\" by Frank Herbert, a classic sci-fi novel set in a distant future where humans have colonized other planets, exploring themes of politics, ecology, and human nature.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "current_messages =  [\n",
    "        SystemMessage(content=\"You are a helpful AI bot that assists a user in choosing the perfect book to read in one short sentence\"),\n",
    "        HumanMessage(content=\"I enjoy mystery novels, what should I read?\")\n",
    "    ]\n",
    "msg = llm.invoke( current_messages)\n",
    "print(msg)\n",
    "current_messages.append(msg)\n",
    "current_messages.append(HumanMessage(content=\"I also like science fiction, what do you recommend?\"))\n",
    "\n",
    "llm.invoke(current_messages).pretty_print()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554bd1cb",
   "metadata": {},
   "source": [
    "## Prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a624ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "512659e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"Tell me one {adjective} joke about {topic}\")\n",
    "inputs = [{\"adjective\": \"funny\", \"topic\": \"cats\"}, {\"adjective\": \"silly\", \"topic\": \"dogs\"}, {\"adjective\": \"hilarious\", \"topic\": \"birds\"}] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "345a34ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the cat join a band?\n",
      "\n",
      "Because it wanted to be the purr-cussionist! (get it?)\n",
      "---\n",
      "Here's one:\n",
      "\n",
      "Why did the dog go to the vet?\n",
      "\n",
      "Because he was feeling ruff!\n",
      "\n",
      "Hope that made you howl with laughter! Do you want another one?\n",
      "---\n",
      "Why did the bird go to the doctor?\n",
      "\n",
      "Because it had a fowl cough! (get it?)\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "\n",
    "for input in inputs:\n",
    "    print(chain.invoke(input).content)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ed2ba0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about cats', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    " (\"system\", \"You are a helpful assistant\"),\n",
    " (\"user\", \"Tell me a joke about {topic}\")\n",
    "])\n",
    "input_ = {\"topic\": \"cats\"}\n",
    "prompt.invoke(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aa4d2319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the day after Tuesday?', additional_kwargs={}, response_metadata={}), AIMessage(content='huh?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Also whats the day after Wednesday?', additional_kwargs={}, response_metadata={})]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The day after Tuesday is Wednesday.\n",
      "\n",
      "And, similarly, the day after Wednesday is Thursday.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "(\"system\", \"You are a helpful assistant\"),\n",
    "MessagesPlaceholder(\"msgs\")  # This will be replaced with one or more messages\n",
    "])\n",
    "\n",
    "input_ = {\"msgs\": [HumanMessage(content=\"What is the day after Tuesday?\"), AIMessage(content=\"huh?\"), HumanMessage(content=\"Also whats the day after Wednesday?\")]}\n",
    "prompt.invoke(input_)\n",
    "print(prompt.invoke(input_))\n",
    "chain = prompt | llm\n",
    "chain.invoke(input_).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7272539",
   "metadata": {},
   "source": [
    "### Output Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "353135fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why don't scientists trust atoms?\",\n",
       " 'punchline': 'Because they make up everything!'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "output_parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],  # Dynamic variables that will be provided when invoking the chain\n",
    "    partial_variables={\"format_instructions\": format_instructions},  # Static variables set once when creating the prompt\n",
    ")\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "chain.invoke({\"query\": joke_query})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f003225f",
   "metadata": {},
   "source": [
    "try to create a loop using a pydantic and chain for multiple jokes with various adjectives and animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3a06c9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic': 'Cats', 'response': \"Knock, knock! Who's there? Purr. Purr who? Purrrr-haps you'll let me in?\"}\n",
      "---\n",
      "{'topic': 'Dogs', 'response': \"Knock, knock! Who's there? Arf. Arf who? Dog gone it, I forgot the punchline!\"}\n",
      "---\n",
      "{'topic': 'Birds', 'response': \"Knock, knock! Who's there? Bird. Bird who? Fowl mood today!\"}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class KnockKnockJoke(BaseModel):\n",
    "    topic: str = Field(description=\"topic of the knock-knock joke\")\n",
    "    response: str = Field(description=\"the knock-knock joke response\")\n",
    "\n",
    "output_parser1 = JsonOutputParser(pydantic_object=KnockKnockJoke)\n",
    "output_parser2 = CommaSeparatedListOutputParser(pydantic_object=KnockKnockJoke)\n",
    "\n",
    "format_instructions1 = output_parser1.get_format_instructions()\n",
    "format_instructions2 = output_parser2.get_format_instructions()\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\nTell me a knock knock joke about {topic}\\n\",\n",
    "    input_variables=[\"query\"],  # Dynamic variables that will be provided when invoking the chain\n",
    "    partial_variables={\"format_instructions\": format_instructions1},  # Static variables set once when creating the prompt\n",
    ")\n",
    "\n",
    "\n",
    "topics = [\"cats\", \"dogs\", \"birds\"]\n",
    "\n",
    "for topic in topics:\n",
    "    chain = prompt | llm | output_parser1\n",
    "    print(chain.invoke({\"topic\": topic}))\n",
    "    print(\"---\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8f0d8494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cat', \"Purr-haps you'll let me in?\"]\n",
      "---\n",
      "['Knock knock', \"who's there?\", 'Ruff', 'Ruff who?', 'Ruff day without treats.']\n",
      "---\n",
      "['Birds', 'chirping', 'flying high']\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\nTell me a knock knock joke about {topic}\\n\",\n",
    "    input_variables=[\"query\"],  # Dynamic variables that will be provided when invoking the chain\n",
    "    partial_variables={\"format_instructions\": format_instructions2},  # Static variables set once when creating the prompt\n",
    ")\n",
    "\n",
    "\n",
    "topics = [\"cats\", \"dogs\", \"birds\"]\n",
    "\n",
    "for topic in topics:\n",
    "    chain = prompt | llm | output_parser2\n",
    "    print(chain.invoke({\"topic\": topic}))\n",
    "    print(\"---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8f7812",
   "metadata": {},
   "source": [
    "### CSV output parser - recipe suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "69f8fad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India: ['Chicken breast', 'Yogurt', 'Lemon juice', 'Garam masala', 'Cumin powder', 'Coriander powder', 'Cayenne pepper', 'Salt', 'Black pepper', 'Vegetable oil', 'Tomato puree', 'Heavy cream', 'Butter', 'Garlic', 'Ginger', 'Onion', 'Fresh cilantro']\n",
      "---\n",
      "Italy: ['Ground beef', 'Onion', 'Garlic', 'Carrot', 'Celery', 'Tomato paste', 'Canned tomatoes', 'Olive oil', 'Salt', 'Black pepper', 'Italian seasoning', 'Red wine (optional)', 'Parmesan cheese', 'Spaghetti']\n",
      "---\n",
      "Mexico: ['Pork shoulder', 'Onion', 'Garlic', 'Cumin', 'Coriander', 'Chili powder', 'Oregano', 'Lime juice', 'Olive oil', 'Pineapple chunks', 'Red onion', 'Cilantro', 'Salt', 'Pepper', 'Tortillas', 'Pita bread', 'Skewers']\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.output_parsers import JsonOutputParser, CommaSeparatedListOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Define Pydantic model\n",
    "class Recipe(BaseModel):\n",
    "    name: str = Field(description=\"name of the recipe\")\n",
    "    cuisine: str = Field(description=\"cuisine type of the recipe\")\n",
    "    ingredients: str = Field(description=\"list of ingredients for the recipe\")\n",
    "\n",
    "# Parsers\n",
    "json_parser = JsonOutputParser(pydantic_object=Recipe)\n",
    "csv_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# Prompts\n",
    "prompt1 = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\nProvide a recipe belonging to a certain cuisine and the required ingredient for the following country: {country}\",\n",
    "    input_variables=[\"country\"],\n",
    "    partial_variables={\"format_instructions\": json_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    template=\"Answer the query.\\n{format_instructions_csv}\\nGiven the recipe name {recipe}, provide a list of required ingredients. Do not add any introduction or explanation.\",\n",
    "    input_variables=[\"recipe\"],\n",
    "    partial_variables={\"format_instructions_csv\": csv_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# Extract recipe name from parsed JSON\n",
    "extract_recipe_name = RunnableLambda(lambda x: {\"recipe\": Recipe(**x).name} if isinstance(x, dict) else {\"recipe\": x.name})\n",
    "\n",
    "\n",
    "# Build chain\n",
    "full_chain = (\n",
    "    prompt1\n",
    "    | llm\n",
    "    | json_parser\n",
    "    | extract_recipe_name\n",
    "    | prompt2\n",
    "    | llm\n",
    "    | csv_parser\n",
    ")\n",
    "\n",
    "# Run chain\n",
    "for country in [\"India\", \"Italy\", \"Mexico\"]:\n",
    "    result = full_chain.invoke({\"country\": country})\n",
    "    print(f\"{country}: {result}\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "726ac7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'country': 'India', 'recipe': 'Butter Chicken', 'ingredients': ['Tomato puree', 'Yogurt', 'Garam masala', 'Cumin powder', 'Coriander powder', 'Red chili powder', 'Salt', 'Ginger paste', 'Garlic paste', 'Lemon juice', 'Butter', 'Vegetable oil', 'Chicken breast', 'Kasoori methi', 'Fresh cilantro']}\n",
      "{'country': 'Italy', 'recipe': 'Spaghetti Carbonara', 'ingredients': ['Spaghetti', 'bacon', 'eggs', 'parmesan cheese', 'black pepper', 'salt']}\n",
      "{'country': 'Mexico', 'recipe': 'Chiles Rellenos', 'ingredients': ['Roasted poblano peppers', 'Queso Oaxaca cheese', 'beaten eggs', 'milk', 'all-purpose flour', 'vegetable oil', 'Salt', 'Pepper', 'Onion', 'Garlic', 'Tomato', 'Fresh cilantro']}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnableMap\n",
    "from langchain_core.output_parsers import StrOutputParser, CommaSeparatedListOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Set up parsers\n",
    "recipe_parser = StrOutputParser()\n",
    "csv_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# Prompts\n",
    "prompt1 = PromptTemplate.from_template(\"Provide only the name of a famous recipe from {country}. Do not add any other text.\")\n",
    "prompt2 = PromptTemplate.from_template(\n",
    "    \"Answer the query.\\n{format_instructions_csv}\\nGiven the recipe name {recipe}, provide a list of required ingredients. Do not add any introduction or explanation.\"\n",
    ")\n",
    "\n",
    "# Add format instructions\n",
    "prompt2 = prompt2.partial(format_instructions_csv=csv_parser.get_format_instructions())\n",
    "\n",
    "# Chain pieces\n",
    "get_recipe_name = prompt1 | llm | recipe_parser\n",
    "store_recipe_name = RunnableLambda(lambda name: {\"recipe\": name.strip()})\n",
    "\n",
    "# Create a full pipeline that keeps all data\n",
    "full_chain = (\n",
    "    RunnableLambda(lambda x: {\"country\": x[\"country\"]})  # Step 1 input\n",
    "    | RunnableLambda(lambda x: {**x, \"recipe\": get_recipe_name.invoke(x)})  # Add recipe name\n",
    "    | RunnableLambda(lambda x: {**x, \"ingredients\": csv_parser.invoke(llm.invoke(prompt2.format(recipe=x[\"recipe\"])))})  # Add ingredients\n",
    ")\n",
    "\n",
    "# Run\n",
    "for country in [\"India\", \"Italy\", \"Mexico\"]:\n",
    "    result = full_chain.invoke({\"country\": country})\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd62278e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agenticrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

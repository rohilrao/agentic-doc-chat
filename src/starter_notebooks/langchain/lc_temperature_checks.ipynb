{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970b01b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "957b2ab3",
   "metadata": {},
   "source": [
    "## LC Prompt Self consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f473b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sequential Requests:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sequential Requests: 100%|██████████| 100/100 [00:10<00:00,  9.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 11.00 seconds\n",
      "Extracted Numbers: [53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "from tqdm import tqdm\n",
    "import time, re\n",
    "\n",
    "llm_main = ChatOllama(\n",
    "    model=\"llama3.1:latest\",\n",
    "    temperature=0.0,\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    api_key=\"ollama\"\n",
    ")\n",
    "\n",
    "random_numbers = []\n",
    "start_time = time.time()\n",
    "\n",
    "for _ in tqdm(range(100), desc=\"Sequential Requests\"):\n",
    "    prompt = HumanMessage(content=\"Give me a random number between 1 and 100.\")\n",
    "    response = llm_main.invoke([prompt])\n",
    "    random_numbers.append(response.content)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Extract numbers\n",
    "numbers = [int(m.group()) for r in random_numbers if (m := re.search(r'\\d+', r))]\n",
    "print(\"Extracted Numbers:\", numbers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a378db6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parallel Requests:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parallel Requests: 100%|██████████| 100/100 [00:06<00:00, 14.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 6.94 seconds\n",
      "Extracted Numbers: [53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import time, re\n",
    "\n",
    "llm_main = ChatOllama(\n",
    "    model=\"llama3.1:latest\",\n",
    "    temperature=0.0,\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    api_key=\"ollama\"\n",
    ")\n",
    "\n",
    "def ask_random_number():\n",
    "    prompt = HumanMessage(content=\"Give me a random number between 1 and 100.\")\n",
    "    try:\n",
    "        response = llm_main.invoke([prompt])\n",
    "        return response.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "NUM_REQUESTS = 100\n",
    "responses = []\n",
    "\n",
    "start_time = time.time()\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = [executor.submit(ask_random_number) for _ in range(NUM_REQUESTS)]\n",
    "    for f in tqdm(as_completed(futures), total=NUM_REQUESTS, desc=\"Parallel Requests\"):\n",
    "        responses.append(f.result())\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Extract numbers\n",
    "numbers = [int(m.group()) for r in responses if (m := re.search(r'\\d+', r))]\n",
    "print(\"Extracted Numbers:\", numbers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f90e38a",
   "metadata": {},
   "source": [
    "## temperature and randomness checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "175d3c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Dog Names:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Dog Names: 100%|██████████| 100/100 [00:16<00:00,  6.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time taken: 16.41 seconds\n",
      "\n",
      "Total names collected: 100\n",
      "Unique names: 1\n",
      "\n",
      "Sample names: [\"The random name I've generated is... **Bramble**!\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# 1. Initialize model\n",
    "llm_main = ChatOllama(\n",
    "    model=\"llama3.1:latest\",\n",
    "    temperature=0.0,  # use 0.0 for deterministic outputs\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    api_key=\"ollama\"\n",
    ")\n",
    "\n",
    "# 2. Collect dog names\n",
    "dog_names = []\n",
    "start_time = time.time()\n",
    "\n",
    "for _ in tqdm(range(100), desc=\"Getting Dog Names\"):\n",
    "    prompt = HumanMessage(content=\"Give me a random name of a dog:\")\n",
    "    response = llm_main.invoke([prompt])\n",
    "    dog_names.append(response.content.strip())\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nTime taken: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# 3. Print summary\n",
    "unique_names = set(dog_names)\n",
    "print(f\"\\nTotal names collected: {len(dog_names)}\")\n",
    "print(f\"Unique names: {len(unique_names)}\")\n",
    "print(\"\\nSample names:\", list(unique_names)[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "991d74b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A question that sparks much debate and speculation! Here are some potential trends and developments that could shape the future of Artificial Intelligence (AI):\\n\\n**Short-term (2023-2030)**\\n\\n1. **Increased adoption**: AI will become more ubiquitous in various industries, including healthcare, finance, transportation, and education.\\n2. **Advancements in natural language processing (NLP)**: AI-powered chatbots and virtual assistants will improve their ability to understand and respond to human language.\\n3. **Rise of edge AI**: As IoT devices proliferate, AI will be deployed closer to the source of data, reducing latency and improving real-time decision-making.\\n4. **Growing importance of explainability**: As AI becomes more pervasive, there will be a greater need for transparent and interpretable AI models.\\n\\n**Mid-term (2030-2040)**\\n\\n1. **Autonomous systems**: Self-driving cars, drones, and robots will become increasingly common, revolutionizing transportation and logistics.\\n2. **AI-powered decision-making**: AI will be used to make complex decisions in areas like finance, healthcare, and law enforcement.\\n3. **Human-AI collaboration**: As AI becomes more capable, humans and machines will work together to achieve shared goals.\\n4. **Development of new AI architectures**: New frameworks, such as graph neural networks and transformers, will emerge to tackle complex tasks.\\n\\n**Long-term (2040-2050)**\\n\\n1. **Singularity-like advancements**: AI may reach a point where it surpasses human intelligence in many domains, leading to significant changes in society.\\n2. **Merging of humans and machines**: Brain-computer interfaces and neural implants could enable humans to enhance their cognitive abilities with AI.\\n3. **AI-driven scientific breakthroughs**: AI will accelerate scientific progress by analyzing vast amounts of data and identifying new patterns and relationships.\\n4. **Rethinking work and leisure**: As AI takes over routine tasks, humans may focus on creative pursuits, education, and personal growth.\\n\\n**Speculative possibilities**\\n\\n1. **Artificial general intelligence (AGI)**: A hypothetical AI system that surpasses human intelligence in all domains.\\n2. **AI-driven space exploration**: AI could play a crucial role in exploring and settling other planets.\\n3. **Synthetic biology**: AI may be used to design new biological systems, leading to breakthroughs in medicine and biotechnology.\\n\\nKeep in mind that these predictions are based on current trends and the imagination of experts in the field. The future of AI is inherently uncertain, and actual developments may differ from these projections.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import time, re\n",
    "\n",
    "# 1. Initialize model\n",
    "llm_main = ChatOllama(\n",
    "    model=\"llama3.1:latest\",\n",
    "    temperature=0.0,  # use 0.0 for deterministic outputs\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    api_key=\"ollama\"\n",
    ")\n",
    "prompt = HumanMessage(content=\"The future of AI is\")\n",
    "response = llm_main.invoke([prompt])\n",
    "response.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "976f3041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here are the results of three independent calculations:\\n\\n**Calculation 1**\\n\\n* Random number 1: 14\\n* Random number 2: 73\\n* Random number 3: 28\\n* Average: (14 + 73 + 28) / 3 = 38.33\\n* Sum: 14 + 73 + 28 = 115\\n* Product: 14 × 73 × 28 = 26,532\\n\\n**Calculation 2**\\n\\n* Random number 1: 91\\n* Random number 2: 19\\n* Random number 3: 46\\n* Average: (91 + 19 + 46) / 3 = 52.67\\n* Sum: 91 + 19 + 46 = 156\\n* Product: 91 × 19 × 46 = 66,474\\n\\n**Calculation 3**\\n\\n* Random number 1: 25\\n* Random number 2: 98\\n* Random number 3: 13\\n* Average: (25 + 98 + 13) / 3 = 45.33\\n* Sum: 25 + 98 + 13 = 136\\n* Product: 25 × 98 × 13 = 32,250\\n\\nNow, let's compare the results:\\n\\n* The average is the most consistent result across all three calculations, with values ranging from 38.33 to 52.67.\\n* The sum is also relatively consistent, with values ranging from 115 to 156.\\n* The product is the least consistent result, with a range of 26,532 to 66,474.\\n\\nBased on these results, I would conclude that the average is the most consistent result across all three calculations.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import time, re\n",
    "\n",
    "# 1. Initialize model\n",
    "llm_main = ChatOllama(\n",
    "    model=\"llama3.1:latest\",\n",
    "    temperature=0.0,  # use 0.0 for deterministic outputs\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    api_key=\"ollama\"\n",
    ")\n",
    "prompt = HumanMessage(content=\"\"\"We are going to perform a series of calculations to determine the most consistent result.\n",
    "            1. Calculate the average of three random numbers between 1 and 100.   \n",
    "            2. Calculate the sum of three random numbers between 1 and 100.\n",
    "            3. Calculate the product of three random numbers between 1 and 100.     \n",
    "            Provide three independent calculations and explanations, then determine the most consistent result.\n",
    "\"\"\")\n",
    "response = llm_main.invoke([prompt])\n",
    "response.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11a53a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Here are the results of three independent calculations:\n",
      "\n",
      "**Calculation 1**\n",
      "\n",
      "* Random number 1: 14\n",
      "* Random number 2: 73\n",
      "* Random number 3: 28\n",
      "* Average: (14 + 73 + 28) / 3 = 38.33\n",
      "* Sum: 14 + 73 + 28 = 115\n",
      "* Product: 14 × 73 × 28 = 26,532\n",
      "\n",
      "**Calculation 2**\n",
      "\n",
      "* Random number 1: 91\n",
      "* Random number 2: 19\n",
      "* Random number 3: 46\n",
      "* Average: (91 + 19 + 46) / 3 = 52.67\n",
      "* Sum: 91 + 19 + 46 = 156\n",
      "* Product: 91 × 19 × 46 = 66,474\n",
      "\n",
      "**Calculation 3**\n",
      "\n",
      "* Random number 1: 25\n",
      "* Random number 2: 98\n",
      "* Random number 3: 13\n",
      "* Average: (25 + 98 + 13) / 3 = 45.33\n",
      "* Sum: 25 + 98 + 13 = 136\n",
      "* Product: 25 × 98 × 13 = 32,250\n",
      "\n",
      "Now, let's compare the results:\n",
      "\n",
      "* The average is the most consistent result across all three calculations, with values ranging from 38.33 to 52.67.\n",
      "* The sum is also relatively consistent, with values ranging from 115 to 156.\n",
      "* The product is the least consistent result, with a range of 26,532 to 66,474.\n",
      "\n",
      "Based on these results, I would conclude that the average is the most consistent result across all three calculations.\n"
     ]
    }
   ],
   "source": [
    "response.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2a43f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_prompt(inputs):\n",
    "    return f\"Tell me a {inputs['adjective']} joke about {inputs['content']}.\"\n",
    "\n",
    "\n",
    "joke_chain = (\n",
    "    RunnableLambda(format_prompt)\n",
    "    | llm_main\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "414536b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Why did the chicken go to the doctor?\n",
      "\n",
      "Because it had fowl breath!\n",
      "\n",
      "Hope that cracked you up! Do you want another one?\n"
     ]
    }
   ],
   "source": [
    "# Run the chain\n",
    "response = joke_chain.invoke({\"adjective\": \"funny\", \"content\": \"chickens\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9223c97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agenticrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
